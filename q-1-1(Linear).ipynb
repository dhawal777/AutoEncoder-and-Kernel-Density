{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random,exp,array, dot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"intrusion_detection/data.csv\"\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_error=[]\n",
    "output1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import mixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork():\n",
    "    def ini(self, gateInput, gateOutput):\n",
    "        self.gateInput = gateInput\n",
    "        self.input_shape = (1,29)\n",
    "        print(self.input_shape)\n",
    "        self.layer_nodes = 14\n",
    "        self.hidden_layers = 1\n",
    "        self.a = {}\n",
    "        self.gateOutput = gateOutput\n",
    "        self.output_shape = (1,29)\n",
    "        print((self.output_shape))\n",
    "\n",
    "    def softmax(self,a):\n",
    "        mx = np.sum(a,axis = 1,keepdims = True)\n",
    "        mx1=mx\n",
    "        ex1 = np.divide(a,mx1)\n",
    "        return ex1\n",
    "    \n",
    "    def initialization(self,):\n",
    "        self.weights = {}   \n",
    "        input1=self.input_shape[1]\n",
    "        layernodes=self.layer_nodes\n",
    "        self.weights[0] = (2 * np.random.random((input1,layernodes)) - 1)*0.01\n",
    "        for i in range(1,self.hidden_layers):\n",
    "            hiddennodes=self.layer_nodes\n",
    "            self.weights[i] = (2 * np.random.random((hiddennodes, hiddennodes)) - 1)*0.01\n",
    "        output1=self.output_shape[1]\n",
    "        self.weights[self.hidden_layers] = (2 * np.random.random((layernodes,output1)) - 1)*0.01\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return x\n",
    "    \n",
    "    def error(self,y1,y_hat):\n",
    "        temp=(y1-y_hat)\n",
    "        y_minus_yhat_square=np.sum(temp*temp,axis = 1)\n",
    "        error1=0.5*y_minus_yhat_square\n",
    "        return np.mean(error1)\n",
    "    \n",
    "    def think(self, x):\n",
    "        input1=x\n",
    "        weight0=self.weights[0]\n",
    "        z=dot(x,weight0)\n",
    "        layer1 = self.sigmoid(z)\n",
    "        return layer1\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        temp=x.shape\n",
    "        w=np.ones(temp)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    def train(self, num_steps):\n",
    "        for x in range(num_steps):\n",
    "            #FORWARD PROP###\n",
    "                initialInput=self.gateInput\n",
    "                weight0=self.weights[0]\n",
    "                z0=dot(initialInput,weight0)\n",
    "                self.a[0] = self.sigmoid(z0)\n",
    "                learning_rate=.0000001\n",
    "                \n",
    "                for i in range(1,self.hidden_layers):\n",
    "                    current_layer_z=self.a[i-1]\n",
    "                    weight_next= self.weights[i]\n",
    "                    z_new=dot(current_layer_z,weight_next)\n",
    "                    self.a[i] = self.sigmoid(z_new)\n",
    "                \n",
    "                last_hidden_layer=self.a[self.hidden_layers-1]\n",
    "                last_weights=self.weights[self.hidden_layers]\n",
    "                output_z=dot(last_hidden_layer,last_weights)\n",
    "                output = self.sigmoid(output_z)\n",
    "                \n",
    "                ##END FORWARD###\n",
    "                ##ERROR##\n",
    "                outputError = -(self.gateOutput - self.softmax(output))\n",
    "                error2=self.error(self.gateOutput,self.softmax(output))\n",
    "                epoch_error.append(error2)\n",
    "                output1=error2\n",
    "                ##END ERROR##\n",
    "                ######BACK PROPOGATION#############\n",
    "                \n",
    "                ##last delta\n",
    "                delta = outputError * self.sigmoid_derivative(output)\n",
    "                hidden_layerLength=self.hidden_layers\n",
    "                out_weights_adjustment = dot(self.a[hidden_layerLength-1].T, delta)\n",
    "                self.weights[hidden_layerLength] -= learning_rate*out_weights_adjustment\n",
    "                for i in np.arange(hidden_layerLength-1,0,-1):\n",
    "                    activation_derivative=self.sigmoid_derivative(self.a[i])\n",
    "                    delta = dot(delta, self.weights[i+1].T)*activation_derivative\n",
    "                    weight_2_adjustment = dot(self.a[i-1].T, delta)\n",
    "                    x1=self.weights[i]-learning_rate*weight_2_adjustment\n",
    "                    self.weights[i] = x1\n",
    "                activation_derivative=self.sigmoid_derivative(self.a[0])\n",
    "                delta = dot(delta, self.weights[1].T) * activation_derivative\n",
    "                weight_1_adjustment = dot(self.gateInput.T, delta)\n",
    "                self.weights[0] -=learning_rate*weight_1_adjustment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = df\n",
    "X= file.drop(['xAttack'],axis=1)\n",
    "X=(X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neural_network = NeuralNetwork()\n",
    "neural_network.ini(X.iloc[0*10000:0*10000+10000],X.iloc[0*10000:0*10000+10000])\n",
    "neural_network.initialization()\n",
    "neural_network.train(50)\n",
    "for i in range(1,11):\n",
    "    print(i)\n",
    "    neural_network.ini(X.iloc[i*10000:i*10000+10000],X.iloc[i*10000:i*10000+10000])\n",
    "    neural_network.train(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_error[-1])\n",
    "# output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=neural_network.think(X)\n",
    "# X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[];\n",
    "x=len(X1);\n",
    "# print(x)\n",
    "for i in range(0,x):\n",
    "    l.append(i)\n",
    "Df = pd.DataFrame(data = X1,\n",
    "                  index=l)\n",
    "data=Df.values\n",
    "k=5\n",
    "c = data.shape[1]\n",
    "# Df\n",
    "\n",
    "Df=pd.concat([Df,file['xAttack']],axis=1)\n",
    "Df.to_csv(\"intrusion_detection/newDataLinear.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"intrusion_detection/newDataLinear.csv\")\n",
    "X2= df.drop(['xAttack'],axis=1)\n",
    "y =pd.DataFrame(df['xAttack'])\n",
    "# X=(X-X.mean())/X.std()\n",
    "labelencoder=LabelEncoder()\n",
    "y=labelencoder.fit_transform(y)\n",
    "y=pd.DataFrame(data=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=X_train.values\n",
    "X_final=pd.concat([X_train,y_train],axis=1)\n",
    "print(type(X_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=X_final.values\n",
    "data2=X_test.values\n",
    "# y_train=pd.concat([y_train,pd.get_dummies(y_train['xAttack'],prefix='xAttack')],axis=1).drop(['xAttack'],axis=1)\n",
    "# y=pd.concat([y,pd.get_dummies(y['label'],prefix='label')],axis=1).drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data,axis = 0)\n",
    "std = np.std(data, axis = 0)\n",
    "k=5\n",
    "c = data.shape[1]\n",
    "print(c)\n",
    "\n",
    "def euclidean_distance(x, y): \n",
    "        return np.sqrt(np.sum((x - y) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.random.randn(k,c)*std+mean\n",
    "n=data.shape[0]\n",
    "\n",
    "centers_old = np.zeros(centers.shape)\n",
    "centers_new = copy.deepcopy(centers)\n",
    "error=0\n",
    "for i in range(k):\n",
    "    temp4=euclidean_distance(centers_new[i],centers_old[i])\n",
    "    error = error+temp4\n",
    "clusters = np.zeros(n)\n",
    "distances = np.zeros((n,k))\n",
    "itr=0\n",
    "ans=[]\n",
    "ans1=[]\n",
    "while error!=0:\n",
    "    for i in range(k):\n",
    "        #getting distance of all point wrt to each cluster center\n",
    "        distances[:,i] = np.linalg.norm(data - centers_new[i], axis=1)\n",
    "    #assigning the cluster to each point whose distance is minimum\n",
    "    clusters = np.argmin(distances, axis = 1)\n",
    "    centers_old = copy.deepcopy(centers_new)\n",
    "    for i in range(k):\n",
    "        #if cluster has no point than reinitialize the process\n",
    "        if(len(data[clusters==i])==0):\n",
    "            clusters = np.zeros(n)\n",
    "            centers = np.random.randn(5,14)\n",
    "            temp=0\n",
    "            for j in range(k):\n",
    "                centers[j]=centers[j]*std+mean\n",
    "            centers_old = np.zeros(centers.shape)\n",
    "            distances = np.zeros((n,k))\n",
    "            centers_new = copy.deepcopy(centers)\n",
    "            for i in range(k):\n",
    "                temp2=euclidean_distance(centers_new[i],centers_old[i])\n",
    "                temp = temp+temp2\n",
    "            error=temp\n",
    "            itr=0\n",
    "            break;\n",
    "        else:\n",
    "            #assigning new center base on assignment of points i.e mean of all the points that belog to cluster==i\n",
    "            centers_new[i] = np.mean(data[clusters == i], axis=0)\n",
    "    temp=0\n",
    "    for i in range(k):\n",
    "        temp3=euclidean_distance(centers_new[i],centers_old[i])\n",
    "        temp = temp+temp3\n",
    "    error=temp\n",
    "    itr=itr+1\n",
    "\n",
    "for i in range(k):\n",
    "    ans.append(np.count_nonzero(clusters==i))\n",
    "    ans1.append(np.count_nonzero(y==i))\n",
    "# print(ans)\n",
    "# print(ans1)\n",
    "\n",
    "def purity_score_Kmeans(y_true, y_pred):\n",
    "    contingency_matrix =metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    num2=np.sum(np.amax(contingency_matrix, axis=0))\n",
    "    print(np.sum(np.amax(contingency_matrix, axis=0)))\n",
    "    print(np.sum(contingency_matrix))\n",
    "    deno=np.sum(contingency_matrix)\n",
    "    return num2/deno \n",
    "\n",
    "\n",
    "print(\"--------------Purity of Kmeans----------------------\")\n",
    "print(purity_score_Kmeans(y_train,clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm=mixture.GaussianMixture(n_components=5).fit(data2)\n",
    "labels = gm.predict(data2)\n",
    "\n",
    "def purity_score_GMM(y_true, y_pred):\n",
    "    contingency_matrix =metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    num2=np.sum(np.amax(contingency_matrix, axis=0))\n",
    "    print(np.sum(np.amax(contingency_matrix, axis=0)))\n",
    "    print(np.sum(contingency_matrix))\n",
    "    deno=np.sum(contingency_matrix)\n",
    "    return num2/deno \n",
    "\n",
    "\n",
    "print(\"----------Gaussian Purity Score------------------\")\n",
    "print(purity_score_GMM(y_test,labels))\n",
    "\n",
    "# contingency_matrix =metrics.cluster.contingency_matrix(y_test, labels)\n",
    "# ans = 0\n",
    "# result=0\n",
    "# cont = contingency_matrix\n",
    "# for i in cont:\n",
    "# #     temp=max(i,axis=0)\n",
    "# #     ans+=temp\n",
    "#     max2=0\n",
    "#     for k in i:\n",
    "#         result=result+k\n",
    "#         if(k>max2):\n",
    "#             max2=k\n",
    "#     ans=ans+max2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data[0:20000]\n",
    "y_test1=y_train[0:20000]\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "k=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.cluster import contingency_matrix\n",
    "def purity_score_HRR(y_true, y_pred):\n",
    "    contingency_matrix =metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    num2=np.sum(np.amax(contingency_matrix, axis=0))\n",
    "    print(np.sum(np.amax(contingency_matrix, axis=0)))\n",
    "    print(np.sum(contingency_matrix))\n",
    "    deno=np.sum(contingency_matrix)\n",
    "    return float(num2)/float(deno) \n",
    "# ans = 0\n",
    "# cont = contingency_matrix(labels,y_test1)\n",
    "# for i in cont:\n",
    "#     temp=max(i)\n",
    "#     ans+=temp\n",
    "# num=ans\n",
    "cluster = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='single')  \n",
    "labels = cluster.fit_predict(data3)  \n",
    "# deno_len=len(data3)\n",
    "# purity = ans/len(data3)\n",
    "print(\"--------------------Purity Score  -----------------\")\n",
    "print(purity_score_HRR(y_test1,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIE CHART##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "sizes = [85.94, 84.19, 53.9]\n",
    "colors = ['gold', 'blue', 'yellow']\n",
    "labels =  'KMeans', 'GMM', 'Hierarchical',\n",
    "explode = (0, 0, 0)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
